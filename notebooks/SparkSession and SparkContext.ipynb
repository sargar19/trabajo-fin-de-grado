{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSession and SparkContext\n",
    "\n",
    "**What are `SparkSession` and `SparkContext`?**\n",
    "\n",
    "The `SparkSession` and `SparkContext` are two important components in Apache Spark, but they serve different purposes and have different functionalities:\n",
    "\n",
    "1. `SparkContext`:\n",
    "    - `SparkContext` is the entry point for Spark functionality in older versions of Spark (prior to Spark 2.0).\n",
    "    - It represents the connection to a Spark cluster and acts as a handle to the cluster.\n",
    "    - It provides the low-level API functionality of Spark, such as creating RDDs (Resilient Distributed Datasets), performing transformations, and executing actions.\n",
    "    - `SparkContext` is used primarily for working with RDDs and is not aware of higher-level structured APIs like DataFrames and Datasets.\n",
    "   \n",
    "\n",
    "2. `SparkSession`:\n",
    "\n",
    "    - `SparkSession` was introduced in Spark 2.0 as a unified entry point for working with structured data and higher-level APIs like DataFrames and Datasets.\n",
    "    - It encapsulates the functionality of `SparkContext` and provides additional capabilities for working with structured data.\n",
    "    - `SparkSession` provides a more user-friendly and intuitive API for working with data in Spark.\n",
    "    - It includes methods for creating DataFrames and Datasets, executing SQL queries, and interacting with various data sources.\n",
    "    - `SparkSession` internally manages a `SparkContext` and automatically creates one if it doesn't exist.\n",
    "    \n",
    "    \n",
    "In summary, `SparkContext` is the older, lower-level entry point for Spark that focuses on RDD-based operations, while `SparkSession` is the newer, higher-level entry point that encompasses the functionality of `SparkContext` while providing a more convenient and structured API for working with data using DataFrames, Datasets, and SQL. In general, it is recommended to use `SparkSession` for most Spark applications, unless you specifically need to work with RDDs or require functionalities not available in the higher-level APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How to build a SparkSession.\n",
    "\n",
    "**Full documentation**: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"Ejemplo1_SparkSession\") \\\n",
    "    .config(\"spark.some.conf ig.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.44:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Ejemplo1_SparkSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fd5fd0a0c70>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How to build a SparkContext.\n",
    "\n",
    "**Full documentation**: https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.SparkContext.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initSC(cores, memory):\n",
    "    \"\"\"\n",
    "    Funci√≥n que inicializa SparkContext\n",
    "    \"\"\"\n",
    "    conf = SparkConf()\\\n",
    "           .setAppName(f\"AppName {cores} cores {memory} mem\")\n",
    "           # .set('spark.cores.max', cores)\\\n",
    "           # .set('spark.executorEnv.PYTHONHASHSEED',\"123\")\\\n",
    "           # .set('spark.executor.memory', memory)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sc.setLogLevel(\"ERROR\")\n",
    "    #sc.addPyFile(\"py.zip\")\n",
    "    return sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.44:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>AppName 0 cores 0 mem</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=AppName 0 cores 0 mem>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = initSC(0,0)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "It is **not recommended** to **create multiple SparkSessions within the same Spark application**. Each SparkSession corresponds to a separate Spark application and represents the entry point for interacting with Spark.\n",
    "\n",
    "The SparkSession encapsulates the SparkContext, SQLContext, HiveContext, and other components required for Spark's functionality. Creating multiple SparkSessions within the same application can lead to conflicts and unexpected behavior.\n",
    "\n",
    "If you have different sections or tasks within your application that require separate configurations or dependencies, it is recommended to **use different SparkContexts within the same SparkSession**. You can create multiple SparkContexts using different names and configurations using the SparkSession.newSession() method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"AppName\").getOrCreate()\n",
    "\n",
    "# Create a new SparkContext within the existing SparkSession\n",
    "sc1 = spark.sparkContext\n",
    "sc2 = spark.newSession().sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, you can have **multiple SparkContexts with different configurations, but they are still part of the same SparkSession**, allowing them to share the same underlying resources.\n",
    "\n",
    "However, keep in mind that creating multiple SparkContexts should be done sparingly and only when necessary, as it can impact performance and resource usage. In most cases, a single SparkSession with a single SparkContext is sufficient for most Spark applications.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para el cluster de Dana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"spark://dana:7077\").setAppName(internal_param[1])./\n",
    "                    setAll([('spark.driver.cores', internal_param[2]),/\n",
    "                            ('spark.driver.memory',internal_param[3]),/\n",
    "                            ('spark.executor.instances', internal_param[4]),/\n",
    "                            ('spark.executor.memory',internal_param[5]),/\n",
    "                            ('spark.executor.cores', internal_param[6])])\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
